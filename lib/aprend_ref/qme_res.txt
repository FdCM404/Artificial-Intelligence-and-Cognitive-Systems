
=== INÍCIO TREINO QME (DEBUG) ===


--- Episódio 1 ---

[DEBUG] Aproveitar (r=0.25) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 0.00 -> -0.75
[DEBUG] Replay: 1 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): 0.00 -> -0.75
[DEBUG] Replay: 2 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 0.00 -> -0.08
[DEBUG] Replay: 3 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Explorar (r=0.08) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 0) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),esquerda): 0.00 -> -0.08
[DEBUG] Replay: 4 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): -0.09 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.29) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.11) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 1) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),cima): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.37) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 0) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),esquerda): -0.10 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.76) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),cima): 0.00 -> -0.79
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.11) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): -0.10 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),cima): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.22) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),direita): 0.00 -> -0.79
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.25) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): -0.10 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.10) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.71) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),baixo): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.12) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
[DEBUG] Explorar (r=0.09) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 0.00 -> -0.79
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.75) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): -0.10 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.31) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.52) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),baixo): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),cima): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.33) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 0.00 -> -0.05
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 3) | Novo estado: (1, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 3),direita): 0.00 -> -0.50
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.73) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 3) | Novo estado: (1, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 3),esquerda): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.76) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 3) | Novo estado: (1, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 3),cima): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Explorar (r=0.12) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 0.00 -> -0.05
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.65) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),esquerda): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),esquerda): 0.00 -> -0.05
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),esquerda): 0.00 -> -0.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.25) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): -0.19 -> -0.25
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.31) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): -0.22 -> -0.22
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.95) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): -0.16 -> -0.18
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): -0.10 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.15) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: direita
[DEBUG] Estado atual: (3, 3) | Novo estado: (3, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((3, 3),direita): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: esquerda
[DEBUG] Estado atual: (3, 3) | Novo estado: (3, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((3, 3),esquerda): 0.00 -> -0.75
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
[DEBUG] Explorar (r=0.19) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 0.00 -> -0.05
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.04) -> ação escolhida: cima
[DEBUG] Estado atual: (4, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((4, 3),cima): 0.00 -> -0.08
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.83) -> ação escolhida: cima
[DEBUG] Estado atual: (3, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),cima): 0.00 -> -0.05
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.73) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 0.00 -> -0.50
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Explorar (r=0.08) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): -0.09 -> -0.17
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.42) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): -0.09 -> -0.10
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.75) -> ação escolhida: esquerda
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((4, 3),esquerda): 0.00 -> -0.50
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((4, 3),baixo): 0.00 -> -0.50
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 0.00 -> 75.00
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 1 concluído | Passos: 47 | Recompensa total: 78.30

--- Episódio 2 ---

[DEBUG] Explorar (r=0.04) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): -1.36 -> -1.37
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.29) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): -0.41 -> -0.41
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
[DEBUG] Explorar (r=0.09) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): -0.34 -> -0.37
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 1.71 -> 5.55
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 9.92 -> 19.72
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 25.65 -> 54.42
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 65.15 -> 70.88
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 85.24 -> 87.91
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.10) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 98.44 -> 99.61
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Episódio 2 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 3 ---

[DEBUG] Aproveitar (r=0.48) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 13.85 -> 27.89
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 35.16 -> 43.66
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
[DEBUG] Aproveitar (r=0.42) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 49.26 -> 53.51
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
[DEBUG] Explorar (r=0.18) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 60.53 -> 64.15
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
[DEBUG] Explorar (r=0.16) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 71.85 -> 72.18
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.06) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.47 -> 80.52
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.73) -> ação escolhida: direita
[DEBUG] Estado atual: (3, 3) | Novo estado: (3, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((3, 3),direita): 76.53 -> 78.97
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.45) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.76 -> 89.86
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.33) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 99.99 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Episódio 3 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 4 ---

[DEBUG] Aproveitar (r=0.80) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 44.43 -> 46.05
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.78) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 51.92 -> 52.23
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.51 -> 58.53
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
[DEBUG] Explorar (r=0.19) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.16 -> 65.21
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.33) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.57 -> 72.62
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.39) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 4 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 5 ---

[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.24 -> 47.30
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.67 -> 52.67
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.86) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 48.12 -> 50.86
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
[DEBUG] Aproveitar (r=0.38) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.58) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 5 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 6 ---

[DEBUG] Aproveitar (r=0.61) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.12) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),direita): 46.30 -> 46.39
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.25) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.83) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),esquerda): 57.51 -> 58.07
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.32) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
[DEBUG] Aproveitar (r=0.86) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.80) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Episódio 6 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 7 ---

[DEBUG] Aproveitar (r=0.27) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Explorar (r=0.19) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.36) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.14) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.17) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: cima
[DEBUG] Estado atual: (3, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),cima): 72.62 -> 72.62
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.15) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Episódio 7 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 8 ---

[DEBUG] Aproveitar (r=0.85) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.62) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.01) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.43) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.10) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 8 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 9 ---

[DEBUG] Explorar (r=0.08) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.42) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),cima): 46.40 -> 46.40
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.79) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 0) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),esquerda): 42.48 -> 42.48
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Explorar (r=0.19) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.02) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 71.71 -> 71.72
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.77) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 71.72 -> 71.72
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.54) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Episódio 9 concluído | Passos: 13 | Recompensa total: 96.10

--- Episódio 10 ---

[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
[DEBUG] Explorar (r=0.09) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Episódio 10 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 11 ---

[DEBUG] Aproveitar (r=0.62) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.67) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Explorar (r=0.02) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.84) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.10) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 71.73 -> 71.73
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.24) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.77) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Episódio 11 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 12 ---

[DEBUG] Aproveitar (r=0.74) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.61) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: cima
[DEBUG] Estado atual: (3, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),cima): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),esquerda): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.12) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.94) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.62) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.16) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 12 concluído | Passos: 12 | Recompensa total: 98.90

--- Episódio 13 ---

[DEBUG] Aproveitar (r=0.83) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Explorar (r=0.05) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.71) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.91) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.19) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Explorar (r=0.17) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Episódio 13 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 14 ---

[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.18) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.15) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),esquerda): 57.74 -> 57.74
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.52) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Explorar (r=0.06) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 14 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 15 ---

[DEBUG] Aproveitar (r=0.54) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
[DEBUG] Explorar (r=0.04) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.45) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.91) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.58) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 65.23 -> 65.25
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.47) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.02) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 15 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 16 ---

[DEBUG] Aproveitar (r=0.41) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.20) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.68) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.78) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),cima): 64.37 -> 64.37
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.29) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.09) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.59) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.87) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 16 concluído | Passos: 9 | Recompensa total: 98.30

--- Episódio 17 ---

[DEBUG] Aproveitar (r=0.24) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.80) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.53) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.07) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=1.00) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 17 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 18 ---

[DEBUG] Aproveitar (r=0.37) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.03) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=cima, r=-1.00, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),esquerda): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.05) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),cima): 64.37 -> 64.37
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 18 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 19 ---

[DEBUG] Aproveitar (r=0.20) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.85) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.24) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.38) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.96) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.53) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.17) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Explorar (r=0.12) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
[DEBUG] Episódio 19 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 20 ---

[DEBUG] Aproveitar (r=1.00) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 0) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),esquerda): 42.48 -> 42.48
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.30) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.50) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 1) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),cima): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Explorar (r=0.03) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.78) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.91) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.20) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.93) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Episódio 20 concluído | Passos: 12 | Recompensa total: 98.90

--- Episódio 21 ---

[DEBUG] Explorar (r=0.03) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=baixo, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.47) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.47) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Explorar (r=0.09) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.42) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.94) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Explorar (r=0.14) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.31) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Episódio 21 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 22 ---

[DEBUG] Aproveitar (r=0.54) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.45) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Episódio 22 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 23 ---

[DEBUG] Aproveitar (r=0.62) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.63) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.20) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
[DEBUG] Aproveitar (r=0.95) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
[DEBUG] Aproveitar (r=0.68) -> ação escolhida: cima
[DEBUG] Estado atual: (4, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((4, 3),cima): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Explorar (r=0.14) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 23 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 24 ---

[DEBUG] Aproveitar (r=0.56) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.85) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=baixo, r=-1.00, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.93) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.16) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.08) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.76) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.09) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.08) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 24 concluído | Passos: 13 | Recompensa total: 97.90

--- Episódio 25 ---

[DEBUG] Aproveitar (r=0.37) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.25) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.82) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.53) -> ação escolhida: cima
[DEBUG] Estado atual: (4, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((4, 3),cima): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.01) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 25 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 26 ---

[DEBUG] Aproveitar (r=0.48) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Explorar (r=0.19) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),cima): 46.41 -> 46.41
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.61) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 1) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),cima): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.63) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.08) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),cima): 64.37 -> 64.37
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=direita, r=-1.00, sn=(1, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.82) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 2) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 2),cima): 64.37 -> 64.37
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.53) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.87) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 71.73 -> 71.73
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.65) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 26 concluído | Passos: 14 | Recompensa total: 95.10

--- Episódio 27 ---

[DEBUG] Aproveitar (r=0.95) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.92) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.62) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.58) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.07) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Episódio 27 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 28 ---

[DEBUG] Aproveitar (r=0.64) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.83) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=esquerda, r=-1.00, sn=(1, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Explorar (r=0.06) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.96) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 28 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 29 ---

[DEBUG] Aproveitar (r=0.62) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.04) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Aproveitar (r=0.29) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.78) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Explorar (r=0.15) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.50) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.31) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Episódio 29 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 30 ---

[DEBUG] Aproveitar (r=0.83) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=esquerda, r=-1.00, sn=(3, 3)
[DEBUG] Aproveitar (r=0.76) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=direita, r=-1.00, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.16) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.80) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.27) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.25) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.94) -> ação escolhida: cima
[DEBUG] Estado atual: (4, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((4, 3),cima): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 30 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 31 ---

[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.59) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 0) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),esquerda): 42.48 -> 42.48
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.29) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.75) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.05) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.32) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.43) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Episódio 31 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 32 ---

[DEBUG] Aproveitar (r=0.97) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.37) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.91) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 1) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),cima): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.94) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.92) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.02) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.08) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.45) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Episódio 32 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 33 ---

[DEBUG] Aproveitar (r=0.92) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.82) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
[DEBUG] Explorar (r=0.10) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.71) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 33 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 34 ---

[DEBUG] Aproveitar (r=0.53) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),cima): 46.41 -> 46.41
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.79) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.38) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.22) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.00) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: esquerda
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((4, 3),esquerda): 89.00 -> 89.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Episódio 34 concluído | Passos: 11 | Recompensa total: 96.30

--- Episódio 35 ---

[DEBUG] Aproveitar (r=0.31) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.17) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.30) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.93) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.55) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),esquerda): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.96) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),esquerda): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.28) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.04) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Episódio 35 concluído | Passos: 13 | Recompensa total: 97.90

--- Episódio 36 ---

[DEBUG] Aproveitar (r=0.40) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
[DEBUG] Aproveitar (r=0.24) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.82) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.95) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.85) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.96) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 36 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 37 ---

[DEBUG] Aproveitar (r=0.30) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.77) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.95) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.85) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),esquerda): 57.74 -> 57.74
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
[DEBUG] Aproveitar (r=0.67) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.33) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 3),direita): 71.73 -> 71.73
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 37 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 38 ---

[DEBUG] Explorar (r=0.06) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.47) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.36) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.87) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.63) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.69) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),baixo): 57.74 -> 57.74
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
[DEBUG] Aproveitar (r=0.26) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.90) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Episódio 38 concluído | Passos: 12 | Recompensa total: 97.10

--- Episódio 39 ---

[DEBUG] Aproveitar (r=0.25) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.92) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.05) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
[DEBUG] Explorar (r=0.19) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.45) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.80) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.15) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.81) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 39 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 40 ---

[DEBUG] Aproveitar (r=0.93) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.36) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.70) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.06) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.12) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.92) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Episódio 40 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 41 ---

[DEBUG] Aproveitar (r=0.54) -> ação escolhida: esquerda
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),esquerda): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.52) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.37) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.61) -> ação escolhida: esquerda
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),esquerda): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.18) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.32) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.28) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.59) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Explorar (r=0.19) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.68) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.46) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 41 concluído | Passos: 13 | Recompensa total: 96.10

--- Episódio 42 ---

[DEBUG] Explorar (r=0.06) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),baixo): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.12) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.48) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.52) -> ação escolhida: cima
[DEBUG] Estado atual: (1, 1) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),cima): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.08) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.00) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.83) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.93) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.74) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.84) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=cima, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.62) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Episódio 42 concluído | Passos: 13 | Recompensa total: 97.90

--- Episódio 43 ---

[DEBUG] Aproveitar (r=0.39) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),cima): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.58) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.39) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Explorar (r=0.17) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.80) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
[DEBUG] Aproveitar (r=0.39) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: esquerda
[DEBUG] Estado atual: (3, 3) | Novo estado: (3, 3) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((3, 3),esquerda): 79.91 -> 79.91
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.47) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Episódio 43 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 44 ---

[DEBUG] Aproveitar (r=0.72) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.60) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=cima, r=-1.00, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Explorar (r=0.17) -> ação escolhida: direita
[DEBUG] Estado atual: (1, 1) | Novo estado: (1, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((1, 1),direita): 51.78 -> 51.78
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=direita, r=-1.00, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.28) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),cima): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.41) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.37) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.84) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.98) -> ação escolhida: cima
[DEBUG] Estado atual: (2, 3) | Novo estado: (1, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),cima): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.18) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 3),baixo): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.56) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.40) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.79) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Episódio 44 concluído | Passos: 13 | Recompensa total: 97.90

--- Episódio 45 ---

[DEBUG] Aproveitar (r=0.43) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.17) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.13) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.63) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((2, 1),esquerda): 57.74 -> 57.74
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.44) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.14) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.01) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.61) -> ação escolhida: cima
[DEBUG] Estado atual: (3, 3) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),cima): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.96) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.53) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.57) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 45 concluído | Passos: 11 | Recompensa total: 98.10

--- Episódio 46 ---

[DEBUG] Aproveitar (r=0.52) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Explorar (r=0.04) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 2), a=esquerda, r=-0.10, sn=(2, 1)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
[DEBUG] Aproveitar (r=0.34) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.04) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.72) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=cima, r=-1.00, sn=(2, 2)
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.43) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.79) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 46 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 47 ---

[DEBUG] Aproveitar (r=0.61) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=cima, r=-1.00, sn=(0, 0)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.91) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.24) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=cima, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=esquerda, r=-1.00, sn=(4, 3)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Explorar (r=0.15) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.87) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.36) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.82) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=cima, r=-0.10, sn=(2, 3)
[DEBUG] Episódio 47 concluído | Passos: 8 | Recompensa total: 99.30

--- Episódio 48 ---

[DEBUG] Aproveitar (r=0.90) -> ação escolhida: cima
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 0) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 0),cima): 41.58 -> 41.58
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
[DEBUG] Explorar (r=0.03) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
[DEBUG] Explorar (r=0.17) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 1) | Novo estado: (0, 1) | Recompensa: -1.00 | Terminado: False
[DEBUG] Q((0, 1),direita): 46.41 -> 46.41
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.65) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.99) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.66) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
[DEBUG] Aproveitar (r=0.54) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Explorar (r=0.07) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=esquerda, r=-1.00, sn=(0, 0)
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.49) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=esquerda, r=-0.10, sn=(0, 0)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.09) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Episódio 48 concluído | Passos: 10 | Recompensa total: 97.30

--- Episódio 49 ---

[DEBUG] Explorar (r=0.00) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Aproveitar (r=0.75) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.35) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=direita, r=-1.00, sn=(1, 1)
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
[DEBUG] Aproveitar (r=0.85) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.88) -> ação escolhida: esquerda
[DEBUG] Estado atual: (2, 3) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),esquerda): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
[DEBUG] Explorar (r=0.18) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Aproveitar (r=0.86) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=esquerda, r=-1.00, sn=(1, 1)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.51) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(2, 3), a=esquerda, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.97) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=esquerda, r=-1.00, sn=(2, 1)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 0), a=direita, r=-0.10, sn=(0, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Episódio 49 concluído | Passos: 10 | Recompensa total: 99.10

--- Episódio 50 ---

[DEBUG] Aproveitar (r=0.99) -> ação escolhida: direita
[DEBUG] Estado atual: (0, 0) | Novo estado: (0, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 0),direita): 47.31 -> 47.31
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(2, 1), a=baixo, r=-1.00, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.23) -> ação escolhida: baixo
[DEBUG] Estado atual: (0, 1) | Novo estado: (1, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((0, 1),baixo): 52.68 -> 52.68
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 3), a=cima, r=-0.10, sn=(1, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
[DEBUG] Aproveitar (r=0.24) -> ação escolhida: baixo
[DEBUG] Estado atual: (1, 1) | Novo estado: (2, 1) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((1, 1),baixo): 58.64 -> 58.64
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Explorar (r=0.00) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 1) | Novo estado: (2, 2) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 1),direita): 65.27 -> 65.27
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 2), a=direita, r=-0.10, sn=(2, 3)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(0, 1), a=baixo, r=-0.10, sn=(1, 1)
[DEBUG] Explorar (r=0.12) -> ação escolhida: direita
[DEBUG] Estado atual: (2, 2) | Novo estado: (2, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 2),direita): 72.63 -> 72.63
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(0, 1), a=direita, r=-1.00, sn=(0, 1)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Aproveitar (r=0.64) -> ação escolhida: baixo
[DEBUG] Estado atual: (2, 3) | Novo estado: (3, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((2, 3),baixo): 80.81 -> 80.81
[DEBUG] Replay: 5 experiências amostradas
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
        s=(1, 1), a=baixo, r=-0.10, sn=(2, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
[DEBUG] Aproveitar (r=0.21) -> ação escolhida: baixo
[DEBUG] Estado atual: (3, 3) | Novo estado: (4, 3) | Recompensa: -0.10 | Terminado: False
[DEBUG] Q((3, 3),baixo): 89.90 -> 89.90
[DEBUG] Replay: 5 experiências amostradas
        s=(0, 0), a=baixo, r=-1.00, sn=(0, 0)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(2, 1), a=direita, r=-0.10, sn=(2, 2)
        s=(4, 3), a=direita, r=100.00, sn=(4, 4)
[DEBUG] Aproveitar (r=0.78) -> ação escolhida: direita
[DEBUG] Estado atual: (4, 3) | Novo estado: (4, 4) | Recompensa: 100.00 | Terminado: True
[DEBUG] Q((4, 3),direita): 100.00 -> 100.00
[DEBUG] Replay: 5 experiências amostradas
        s=(2, 1), a=cima, r=-0.10, sn=(1, 1)
        s=(2, 3), a=baixo, r=-0.10, sn=(3, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
        s=(1, 3), a=baixo, r=-0.10, sn=(2, 3)
        s=(3, 3), a=baixo, r=-0.10, sn=(4, 3)
[DEBUG] Episódio 50 concluído | Passos: 8 | Recompensa total: 99.30

==================================================
Labirinto 5x5
==================================================
 S  ↓  #  #  # 
 #  ↓  #  ↓  # 
 #  →  →  ↓  # 
 #  #  #  ↓  # 
 #  #  #  →  F 
==================================================

=== TREINO QME CONCLUÍDO (DEBUG) ===

